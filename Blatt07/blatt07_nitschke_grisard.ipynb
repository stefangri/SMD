{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "siehe PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Was beschreibt die Loss-Funktion?\n",
    "\n",
    "b) Wie kann die Lossfunktion minimiert werden?\n",
    "\n",
    "c) Welche Funktion haben die Aktivierungsfunktionen bzw. welches Problem wird \n",
    "   durch diese gelöst? Nennen Sie drei gängige Aktivierungsfunktionen.\n",
    "   \n",
    "d) Was ist ein Neuron?\n",
    "\n",
    "e) Nennen Sie drei Anwendungsbeispiele für Neuronale Netze und beschreiben Sie \n",
    "   kurz warum sie für diese Beispiele besonders geeignet sind.\n",
    "   \n",
    "Literaturtipp: http://www.informatik.uni-ulm.de/ni/Lehre/SS04/ProsemSC/ausarbeitungen/Ruland.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Die Lossfunktion beschreibt den Informationsverlust, der bei einem Schnitt hervorgerufen wird. Da die Lossfunktion\n",
    "   unter anderem die Diskrepanz zwischen vorhergesagten und beobachteten Daten beschreibt, soll diese minimiert werden.\n",
    "   \n",
    "b) Allgemein durch analytisches (iteratives) Vorgehen. Zum Beispiel durch die lineare Klassifikation \n",
    "   --> bestimme das W, welches die Kostenfunktion minimiert.\n",
    "   \n",
    "c) Die Aktivierungsfunktion ist wichtig für die Anregung eines weiteren Neurons. Die Aktivierungsfunktion, der Output\n",
    "   eines Neurons sowie der verwendete Schwellenwert bestimmen, ob die Information an ein gewisses Neuron des nächsten\n",
    "   Layers weitergeleitet wird. Dabei muss die Aktivierungsfunktion auch immer zum gewünschten Output des Neurons passen. \n",
    "   Mithilfe der Aktivierungsfunktion kann der Raum verzerrt werden, sodass im verzerrten Raum gerade Schnitte gesetzt werden.\n",
    "   Im unverzerrten Raum können dann zum Beispiel Werte getrennt werden, die gemäß einer Spirale angeordnet sind.\n",
    "   - ReLu\n",
    "   - Leaky ReLu\n",
    "   - tanh\n",
    "   \n",
    "d) Ein Neuron besteht grundlegend aus 3 Teilen. Den ersten Teil bildet die sogenannte Propagierungsfunktion, welche \n",
    "   mithilfe der Gewichte die Netzeingabe bestimmt. Mit der oben beschriebenen Aktivierungsfunktion wird dann der interne\n",
    "   Zustand des Neurons festgelegt. Mithilfe der Ausgabefunktion wird letztendlich der Output eines Neurons bestimmt.\n",
    "   \n",
    "e) \n",
    "   - Bildverarbeitung: \n",
    "   Mithilfe neuronaler Netze können die enormen Datenmengen durch Reduktion der Dimensionalität \n",
    "   schneller und leichter verarbeitet werden. \n",
    "                       \n",
    "   - Klassifizierung: \n",
    "   Mithilfe der neuronalen Netze kann die Trennung und Klassifizierung von Datenpunkten effizient \n",
    "   durchgeführt werden. Ein mögliches Anwendungsgebiet wäre zum Beispiel die Teichenphysik, da aus \n",
    "   einer riesigen Menge an Events nur bestimmte Events gefiltert werden sollen. Für diese Filterung\n",
    "   kann dann zum Beispiel ein neuronales Netz trainiert werden.\n",
    "                \n",
    "   - Prozeßoptimierung: \n",
    "   Neuronale Netze können zum Beispiel in der Wirtschaft genutzt werden, um bestimmte Prozeßparameter\n",
    "   zu optimieren um so die Effizienz zu steigern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ Dimensionen der Größen gekennzeichnet durch dim($\\dots$) = (m, n) - m Zeilen, n Spalten<br>\n",
    " - dim($x_i$) = (M, 1) \n",
    " - dim(C) = (1, 1)\n",
    " - dim(W) = (K, M)\n",
    " - dim(b) = (K, 1)\n",
    " - dim($\\nabla_WC$) = (M, K)\n",
    " - dim($\\nabla_fC$) = (K, 1)\n",
    " - dim($\\frac{\\partial f_{k,i}}{\\partial W}$) = (M, K)\n",
    " - dim($\\frac{\\partial f_{k,i}}{\\partial b}$) = (K, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ Implementierung der linearen Klassifikation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "P_0 = pd.read_hdf('populationen.hdf5', key = 'P_0')\n",
    "P_1 = pd.read_hdf('populationen.hdf5', key = 'P_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add labels \n",
    "P_0['label'] = np.zeros_like(P_0.x)\n",
    "P_1['label'] = np.ones_like(P_1.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine data sets\n",
    "P = pd.concat([P_0, P_1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear classification model \n",
    "def f(x, W, b):\n",
    "    return np.matmul(W, x.T) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update matrix and vector b, learning rate h = 0.5\n",
    "def update(W, grad_W, b, grad_b, h = 0.5):\n",
    "    W = W - h * grad_W\n",
    "    b = b - h * grad_b\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion 'grad_f' berechnet den Gradienten $\\nabla_f C$, der für die Berechnung der Gradienten \n",
    "$\\nabla_W C$ und $\\nabla_b C$ benötigt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f(x, f, labels):\n",
    "    labels = labels.values\n",
    "    m = np.shape(x)[0] # number of examples\n",
    "    x = np.matrix(x.values)\n",
    "    \n",
    "    #all different classes (here only 0 and 1)\n",
    "    classes = np.unique(labels)  \n",
    "    \n",
    "    #generate a matrix representing the real class distribution\n",
    "    truth = np.zeros_like(x.T)\n",
    "    mask = []\n",
    "    for i in range(len(classes)):\n",
    "        mask.append(labels == classes[i]) \n",
    "    truth[~np.array(mask)] = 1  \n",
    "    \n",
    "    #calculate the gradient \n",
    "    grad_f = 1 / m * (np.exp(f) / np.exp(f).sum(axis = 0) - truth)\n",
    "    \n",
    "    return grad_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen zur Berechnung der Gradienten $\\nabla_W C$ und $\\nabla_b C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_W(x, f, labels):\n",
    "    grad_F = grad_f(x, f, labels)\n",
    "    grad_W = np.matmul(grad_F, x)\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_b(x, f, labels):\n",
    "    grad_F = grad_f(x, f, labels)\n",
    "    grad_b = grad_F.sum(axis = 1)\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verwende nun die Funktionen um für 100 Epochen zu trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose initial W and b randomly \n",
    "W = np.matrix(np.random.rand(2, 2)) \n",
    "b = np.matrix(np.random.rand(2, 1))\n",
    "x = P.drop(columns = 'label')\n",
    "\n",
    "for i in range(100):\n",
    "    f_init = f(x, W, b)\n",
    "    W, b = update(W = W, grad_W = grad_W(x, f_init, P.label), \n",
    "                  b = b, grad_b = grad_b(x, f_init, P.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ Die Geradengleichung (hier nur Spezialfall für 2 Klassen): \n",
    "$$\n",
    "    y(x) = \\frac{1}{W_{12} - W_{22}} \\left\\{(W_{21} - W_{11}) x + b_{2} - b_{1} \\right\\}\n",
    "$$\n",
    "Ergibt sich aus der Bedingung $f_1 = f_2$, da entlang der Gerade der Score für beide Klassen gleich ist. Die Gerade trennt die beiden Populationen. <br>\n",
    "Grafische Darstellung des Ergebnisses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, W, b):\n",
    "    return 1 / (W[0, 1] - W[1, 1]) * ((W[1, 0] - W[0, 0])*x + b[1] - b[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "plt.scatter(P_0.x, P_0.y, s = 1, label = 'Population 0')\n",
    "plt.scatter(P_1.x, P_1.y, s = 1, label = 'Population 1')\n",
    "xplot = np.linspace(-15, 20, 100)\n",
    "plt.xlim(xplot[0], xplot[-1])\n",
    "plt.ylim(lin(xplot[0], W, b), lin(xplot[-1], W, b))\n",
    "plt.plot(xplot, lin(xplot, W, b).T, color = 'r', label = 'Gerade $y(x)$')\n",
    "plt.legend()\n",
    "plt.xlabel('x', )\n",
    "plt.ylabel('y')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
