{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ Da beim _kNN_-Algorithmus muss die (euklidische) Norm zwischen den Test und Traningsdatenpunkten berechnet werden. Weichen die Größenordnungen der Attribute stark voneinander ab, kann es z.B. \n",
    "zu Rundungsfehlern kommen. Zudem ist der Abstandsbegriff sinnvoller, wenn die einzelnen Skalen normiert werden, da dann relative Abstände für alle Attribute gleich gewichtet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b)__ Der Algorithmus wird als _lazy learner_ bezeichnet, da er eigentlich gar nicht lernt. Der Trainingsdatensatz wird einfach nur abgespeichert und für jeden neuen Testdatensatz die k nächsten Nachbarn neu berechnet werden. Die Laufzeit der Lernphase ist verschwindend klein, während die Anwendungsphase lang dauert. Bei der _SVM_ ist es genau andersherum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c)__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from ml import plots\n",
    "%matplotlib inline\n",
    "from graphviz import Source\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klassenstruktur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.training_data = X \n",
    "        self.training_labels = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        #calculate the eucleadian distance (ignore the root, \n",
    "        #cause its a monoton function)\n",
    "        #between each test event and each training event\n",
    "        distance = (-2 * np.dot(X, self.training_data.T) \n",
    "                    + np.sum(X**2, axis=1)[:, np.newaxis] \n",
    "                    + np.sum(self.training_data**2, axis=1)[np.newaxis, :])\n",
    "        \n",
    "        #generate matrix with labels of the k nearest neighbours\n",
    "        labels = self.training_labels.values[(np.argsort(distance))[:, :self.k]]\n",
    "        \n",
    "        #most common label of the k nearest neighbours as \n",
    "        #prediction for each test event\n",
    "        prediction = []\n",
    "        for i in range(np.shape(labels)[0]):\n",
    "            count = Counter(labels[i, :])\n",
    "            prediction.append(count.most_common(1)[0][0])\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ Bringe zunächst die Daten in die benötigte Form: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read hdf5 file\n",
    "neutrino_signal = pd.read_hdf('NeutrinoMC.hdf5', key = 'Signal')\n",
    "\n",
    "#select the accepted events\n",
    "neutrino_signal = neutrino_signal[neutrino_signal.AcceptanceMask]\n",
    "\n",
    "#delete the energy and the acceptancemask (not relevant for this task)\n",
    "neutrino_signal = neutrino_signal.drop(columns = ['Energy', 'AcceptanceMask'])\n",
    "\n",
    "#reset the index of the DataFrame\n",
    "neutrino_signal = neutrino_signal.reset_index(drop = True)\n",
    "\n",
    "#add label to the signal events\n",
    "neutrino_signal['label'] = Series(data = ['signal' for i in neutrino_signal.x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das gleiche für die Untergrundevents: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrino_background = pd.read_hdf('NeutrinoMC.hdf5', key = 'Background')\n",
    "neutrino_background['label'] = Series(data = ['background' for i in \n",
    "                                              neutrino_background.x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Funktion um einen gewünschten gemischten Datensatz aus Signal und Untergrund zu erstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_sample(signal_events, background_events, n_signal, n_background):\n",
    "    data_set = pd.concat([background_events.sample(n_background), \n",
    "                          signal_events.sample(n_signal)], \n",
    "                         ignore_index=True)\n",
    "    X = data_set.drop(columns = 'label')\n",
    "    y = data_set['label']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen für Reinheit usw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reinheit\n",
    "def precision(true_pos, false_pos):\n",
    "    return len(true_pos) / (len(true_pos) + len(false_pos))\n",
    "\n",
    "#Effizienz\n",
    "def recall(true_pos, false_neg):\n",
    "    return len(true_pos) / (len(true_pos) + len(false_neg))\n",
    "\n",
    "#Signifikanz\n",
    "def significance(true_pos, false_pos):\n",
    "    return len(true_pos) / np.sqrt(len(true_pos) + len(false_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generiere den Trainings- und Testdatensatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, y_training = mix_sample(neutrino_signal, neutrino_background, 5000, 5000)\n",
    "X_test, y_test = mix_sample(neutrino_signal, neutrino_background, 10000, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab hier ist das Vorgehen für die Aufgabenteile d)-f) analog, wesegen eine Funktion für die Prozedur geschrieben wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procedure(k, X_training, y_training, X_test, y_test):\n",
    "    #use the knn algorithm \n",
    "    knn = KNN(k = k)\n",
    "    knn.fit(X = X_training, y = y_training)\n",
    "    prediction = knn.predict(X = X_test)\n",
    "    \n",
    "    #add results to test data set\n",
    "    X_test['prediction'] = Series(prediction)\n",
    "    X_test['truth'] = y_test\n",
    "    \n",
    "    \n",
    "    #calculate true positive etc\n",
    "    true_positive  = X_test[(X_test.truth == 'signal') & \n",
    "                            (X_test.prediction == 'signal')]\n",
    "    \n",
    "    true_negative  = X_test[(X_test.truth == 'background') & \n",
    "                            (X_test.prediction == 'background')]\n",
    "    \n",
    "    false_positive = X_test[(X_test.truth == 'background') & \n",
    "                            (X_test.prediction == 'signal')]\n",
    "    \n",
    "    false_negative = X_test[(X_test.truth == 'signal') & \n",
    "                            (X_test.prediction == 'background')]\n",
    "    \n",
    "    #calculate precision etc\n",
    "    precision_knn = precision(true_positive, false_positive)\n",
    "    recall_knn = recall(true_positive, false_negative)\n",
    "    significance_knn = significance(true_positive, false_positive)\n",
    "\n",
    "    print(f'Reinheit: \\t{precision_knn}\\nEffizienz: \\t{recall_knn}\\nSignifikanz: \\t{significance_knn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier nun KNN Algorithmus mit $k = 10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure(10, X_training, y_training, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ Nun $log_{10}(N)$ statt $N$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrino_signal_e = neutrino_signal\n",
    "neutrino_signal_e['log10NumberOfHits'] = np.log10(neutrino_signal['NumberOfHits'])\n",
    "neutrino_signal_e = neutrino_signal_e.drop(columns = 'NumberOfHits')\n",
    "\n",
    "neutrino_background_e = neutrino_background\n",
    "neutrino_background_e['log10NumberOfHits'] = np.log10(neutrino_background['NumberOfHits'])\n",
    "neutrino_background_e = neutrino_background_e.drop(columns = 'NumberOfHits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun das gleiche wie oben: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, y_training = mix_sample(neutrino_signal_e, \n",
    "                                    neutrino_background_e, \n",
    "                                    5000, 5000)\n",
    "X_test, y_test = mix_sample(neutrino_signal_e, \n",
    "                            neutrino_background_e, \n",
    "                            10000, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure(10, X_training, y_training, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kommentar: Alle Attribute werden besser. Wie oben erwähnt ist dies durch eine Umskalierung des Attributs 'Hits' bedingt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f)__ Nun das ganze mit $k = 20$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training, y_training = mix_sample(neutrino_signal, neutrino_background, 5000, 5000)\n",
    "X_test, y_test = mix_sample(neutrino_signal, neutrino_background, 10000, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure(20, X_training, y_training, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kommentar: Alle Attribute werden schlechter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der handschriftliche Teil der Aufgabe befindet sich am Ende der pdf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DataFrame()\n",
    "X['temperature'] = Series([29.4, 26.7, 28.3, 21.1, 20, 18.3, 17.8, 22.2, 20.6, 23.9, 23.9, 22.2, 27.2, 21.7])\n",
    "X['report'] = Series([2, 2, 1, 0, 0, 0, 1, 2, 2, 0, 2, 1, 1, 0])\n",
    "X['humidity'] = Series([85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80])\n",
    "X['wind'] = Series([False, True, False, False, False, True, True, False, False, False, True, True, False, True])\n",
    "#X\n",
    "\n",
    "y = DataFrame({'football': Series([False, False, True, True, True, False, True, False, True, True, True, True, True, False])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y, splits = [[True for y in range(len(y))]]):\n",
    "    H = []\n",
    "    for split in splits:\n",
    "        entropy = 0\n",
    "        values = Counter(y[split]).most_common()\n",
    "        for value in values:\n",
    "            entropy -= value[1] / len(y[split]) * np.log2(value[1] / len(y[split]))\n",
    "        H.append(entropy)    \n",
    "    return np.array(H)      \n",
    "\n",
    "def information_gain(y, splits):\n",
    "    return entropy(y) - entropy(y, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entropy(y.football))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_split = [1, 2, 3]\n",
    "report_splits = [X.report <= report for report in report_split]\n",
    "\n",
    "report_information_gain = information_gain(y.football, report_splits)\n",
    "plt.plot(report_split, report_information_gain, 'ro')\n",
    "plt.xticks([1, 2, 3])\n",
    "plt.xlabel('Schnitt Wetterbericht')\n",
    "plt.ylabel('Informationsgewinn')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity_split = np.linspace(min(X.humidity), max(X.humidity), 200)\n",
    "humidity_splits = [X.humidity <= H for H in humidity_split]\n",
    "\n",
    "H_information_gain = information_gain(y.football, humidity_splits)\n",
    "plt.plot(humidity_split, H_information_gain, 'r-')\n",
    "plt.xlabel('Schnitt-Luftfeuchtigkeit')\n",
    "plt.ylabel('Informationsgewinn')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_split = np.linspace(min(X.temperature), max(X.temperature), 200)\n",
    "temperature_splits = [X.temperature <= T for T in temperature_split]\n",
    "\n",
    "\n",
    "T_information_gain = information_gain(y.football, temperature_splits)\n",
    "plt.plot(temperature_split, T_information_gain, 'r-')\n",
    "plt.xlabel('Schnitt-Temperatur')\n",
    "plt.ylabel('Informationsgewinn')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Attribut Temperatur eignet sich am besten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "discrete_cmap = ListedColormap(['xkcd:red', 'xkcd:blue'])\n",
    "clf = DecisionTreeClassifier(max_depth=10, criterion='entropy')\n",
    "clf.fit(X, y) \n",
    "tree.export_graphviz(clf, out_file = 'tree.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie sollten nicht-numerische Datentypen wie beispielsweise Strings vor der Analyse\n",
    "behandelt werden müssen?\n",
    "b) Kann es hilfreich sein Attribute zu normieren? Wenn ja, wieso?\n",
    "c) Wie kann mit Lücken in den Daten oder NaNs und Infs verfahren werden?\n",
    "d) Was ist beim Zusammenführen von Datensätzen zu beachten?\n",
    "e) Welche Attribute sollten vor dem Trainieren des Klassifizierers aus dem Datensatz\n",
    "entfernt werden. Wie kann dabei eine Reduktion redundanter Informationen er-\n",
    "reicht werden? Was mum\n",
    "ss speziell bei simulationsbasierten Methoden berücksichtigt\n",
    "werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a)__ Aus den Strings sollten vergleichbare Attribute generiert werden, z.B. die Länge oder die Anzahl an bestimmten Buchstaben. Falls die Strings nur eine Eigenschaft angeben, kann diese einer Zahl zugeordnet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b)__ Dies kann z.B. für den kNN Algorithmus relevant sein, wie oben bereits erklärt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c)__ Man kann diese Attribute oder den jeweiligen Datenpunkt einfach weglassen. Oder interpretieren, was z.B. Infs in dem Kontext bedeuten und dann entsprechend einen anderen Wert zuweisen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d)__ Die Datensätze müssen über die selben Attribute verfügen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e)__ Attribute, die für alle Daten gleich sind enthalten z.B. keinerlei Information. Manche Daten stehen vielleicht in einem funktionellen Zusammenhang zueinander. Einer Reduktion der Attribute kann z.B. mit der PCA erreicht werden. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
